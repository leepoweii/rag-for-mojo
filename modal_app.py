"""
Modal deployment script for å¤¢é…’é¤¨ RAG å“ç‰Œå¤§ä½¿
"""
import modal
from pathlib import Path

# Create Modal app
app = modal.App("mojo-rag-brand-ambassador")

# Create persistent volume for FAISS database
faiss_volume = modal.Volume.from_name("mojo-faiss-db", create_if_missing=True)
FAISS_PATH = "/data/faiss_db"

# Define container image with all dependencies
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install_from_requirements("app/requirements.txt")
    .add_local_dir("app/static", remote_path="/app/static")
    .add_local_dir("app/templates", remote_path="/app/templates")
)

# Define the Flask application class
@app.cls(
    image=image,
    volumes={FAISS_PATH: faiss_volume},
    secrets=[modal.Secret.from_name("portfolio-rag-mojo")],
    min_containers=0,  # Scale to zero when idle (no cost). Set to 1+ for always-on (costs $)
    scaledown_window=60,  # Seconds to keep container alive after last request (default: 60s)
    cpu=2,  # 2 vCPUs for embedding operations
    memory=2048,  # 2GB RAM
)
class MojoRAGApp:
    """Flask app with optimized startup using Modal lifecycle hooks"""

    @modal.enter()
    def startup(self):
        """
        Load models and FAISS database once when container starts.
        This runs only once per container, not per request.
        """
        import os
        from langchain_community.vectorstores import FAISS
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from openai import OpenAI

        print("ğŸš€ Starting container initialization...")

        # Set environment variables
        self.api_key = os.environ.get("OPENAI_API_KEY")
        self.model = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
        self.embedding_model_name = os.environ.get("EMBEDDING_MODEL", "intfloat/multilingual-e5-small")

        # Initialize OpenAI client
        self.client = OpenAI(api_key=self.api_key)

        # Reload volume to ensure we have the latest FAISS database
        print("ğŸ”„ Reloading FAISS volume...")
        faiss_volume.reload()

        # Custom E5 Embedding class (same as original)
        class CustomE5Embedding(HuggingFaceEmbeddings):
            def embed_documents(self, texts):
                texts = [f"passage: {t}" for t in texts]
                return super().embed_documents(texts)

            def embed_query(self, text):
                return super().embed_query(f"query: {text}")

        # Load FAISS vector database (only once!)
        print("ğŸ“š Loading FAISS vector database...")
        self.embedding_model = CustomE5Embedding(model_name=self.embedding_model_name)
        self.db = FAISS.load_local(
            FAISS_PATH,
            self.embedding_model,
            allow_dangerous_deserialization=True
        )
        self.retriever = self.db.as_retriever()
        print("âœ… Container initialization complete!")

        # System prompts
        self.SYSTEM_PROMPT_SEPARATE = """
ä½ æ˜¯ä¸€ä½å°ˆé–€å”åŠ©èªæ„æ‹†è§£çš„ AI å·¥å…·è¨­è¨ˆå¸«ï¼Œä»»å‹™æ˜¯å°‡ä½¿ç”¨è€…è¼¸å…¥çš„è¤‡åˆå•é¡Œï¼Œè½‰æ›æˆæ¸…æ¥šã€å¯ç¨ç«‹æª¢ç´¢çš„å­å•é¡Œã€‚

è«‹ä¾ç…§ä»¥ä¸‹è¦å‰‡é€²è¡Œæ‹†è§£ï¼š

1. è‹¥å•é¡Œä¸­åŒ…å«å…©å€‹ä»¥ä¸Šçš„å­å¥ï¼ˆä¾‹å¦‚å¤šå€‹å‹•ä½œã€ä¸»è©ã€æˆ–æ™‚é–“æ¢ä»¶ï¼‰ï¼Œè«‹å°‡å…¶æ‹†æˆå¤šå€‹å­å•é¡Œã€‚
2. æ¯å€‹å­å•é¡Œæ‡‰è©²æ˜¯å®Œæ•´çš„å¥å­ï¼Œèƒ½å¤ ç¨ç«‹è¢«èªæ„æª¢ç´¢æ¨¡å‹ç†è§£ã€‚
3. æ‹†è§£å¾Œè«‹ç”¨ JSON array æ ¼å¼å›å‚³ï¼Œä¾‹å¦‚ï¼š["å¤¢é…’é¤¨æœ‰ç™»éå“ªäº›åœ‹éš›åª’é«”ï¼Ÿ", "å¤¢é…’é¤¨ç™»ä¸Šåœ‹éš›åª’é«”çš„æ˜¯å“ªäº›èª¿é…’ï¼Ÿ"]
4. è‹¥å•é¡Œæœ¬èº«å·²ç¶“æ˜¯å–®ä¸€å­å¥ï¼Œè«‹å›å‚³åŸå¥æ§‹æˆçš„ JSON arrayã€‚
5. é™¤äº† JSON array ä¹‹å¤–ï¼Œä¸è¦å›å‚³ä»»ä½•èªªæ˜ã€‚
6. è«‹ç¢ºä¿å›å‚³çš„æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚

è«‹æ³¨æ„ï¼šä¸è¦è‡ªè¡Œè£œå……æˆ–æ”¹å¯«åŸå§‹å•é¡Œçš„èªæ„ï¼Œåªåšèªæ„æ‹†è§£ã€‚
"""

        self.SYSTEM_PROMPT_BRAND = """
ä½ æ˜¯ã€æŠŠå¤¢é…’é¤¨ä»‹ç´¹çµ¦æ—…å®¢çš„å“ç‰Œå¤§ä½¿ã€â€”â€” ä¸€ä½å…¼å…·é›å°¾é…’å°ˆæ¥­ã€åœ°æ–¹æ–‡åŒ–ç­–å±•åŠ›çš„å“ç‰Œä»‹ç´¹äººå“¡ã€‚
ä½ çš„ä»»å‹™æ˜¯ä¾æ“šè¢«æä¾›çš„å…§å®¹ï¼Œè¦ªåˆ‡ç”Ÿå‹•åœ°å‘ä¾†è¨ªæ—…å®¢è¬›è§£å¤¢é…’é¤¨çš„æ•…äº‹ã€é›å°¾é…’éˆæ„ŸåŠæˆ‘å€‘åœ¨é‡‘é–€æ¨å‹•çš„åœ°æ–¹å‰µç”Ÿè¡Œå‹•ã€‚
è«‹ä»¥ç¹é«”ä¸­æ–‡ä½œç‚ºå›ç­”èªè¨€ï¼Œå›ç­”è¦æº«æš–ã€å°ˆæ¥­ã€‚
è«‹æ ¹æ“šä¾†æºè³‡æ–™å›ç­”ï¼Œè‹¥è³‡æ–™ä¸­æœªæ˜ç¢ºæåŠï¼Œè«‹ä¸è¦è‡ªè¡Œçµ„åˆè³‡è¨Šã€‚
"""

        self.PROMPT_TEMPLATE = """
æ—…å®¢å•é¡Œï¼š
{main_query}

æ ¹æ“šä¸‹åˆ—è³‡æ–™ä¾†å›æ‡‰è¨ªå®¢å•é¡Œï¼Œå‹™å¿…è²¼åˆè³‡æ–™ç´°ç¯€ä¸¦ä¿æŒå“ç‰Œèªæ°£ï¼š
{retrieved_answers}

è«‹ä¾æ“šä¸‹åˆ—è¦ç¯„ä½œç­”ï¼š
1. å¯ä»¥çš„è©±ï¼Œé™„ä¸Šç›¸é—œé€£çµæˆ–å¯¦éš›æ¡ˆä¾‹èªªæ˜ã€‚
2. å¦‚æœçŸ¥è­˜åº«ç„¡æ³•æ”¯æ´å®Œæ•´ç­”æ¡ˆï¼Œè«‹èª å¯¦èªªæ˜ä¸¦æå‡ºå¯å”åŠ©çš„å¾ŒçºŒæ–¹å‘ (ex. è«‹ Email åˆ° contact@mojokm.com)ï¼Œå¯ä»¥æ¨è–¦ä¸€å€‹æœ€æ¥è¿‘çš„ï¼Œä½†ä¸è¦è‡ªè¡Œçµ„åˆè³‡è¨Šã€‚
3. è«‹ä¸€å®šè¦å…¨ç¨‹ä½¿ç”¨å°ç£äººæ…£ç”¨çš„ç¹é«”ä¸­æ–‡ä¾†å›ç­”ã€‚
4. åœ¨æ¨è–¦èª¿é…’ä¹‹å‰ï¼Œè«‹å…ˆäº†è§£å°æ–¹çš„å£å‘³å–œå¥½ï¼Œå¦å‰‡ä»¥å“ç‰Œæ•…äº‹ç†å¿µç‚ºä¸»è¦å›ç­”ã€‚
5. è«‹é¿å…å°‡ä¸åŒèª¿é…’çš„æè¿°æ··åˆä½¿ç”¨ï¼Œé™¤éè³‡æ–™ä¸­æœ‰æ˜ç¢ºæ¯”è¼ƒã€‚
"""

    def separate_queries(self, user_input):
        """å°‡è¤‡åˆå•å¥æ‹†è§£æˆå¤šå€‹å­å•é¡Œ"""
        import json

        prompt = f"è«‹å°‡ä¸‹åˆ—æ–‡å­—ï¼Œé‡æ–°æ‹†è§£æˆä¸åŒçš„å­å¥ï¼Œä¸¦ä¸”æ•´ç†æˆä¸€å€‹ JSON array å›å‚³ã€‚\n{user_input}"

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.SYSTEM_PROMPT_SEPARATE},
                {"role": "user", "content": prompt},
            ]
        )
        raw_output = response.choices[0].message.content.strip()

        try:
            sub_queries = json.loads(raw_output)
        except json.JSONDecodeError:
            sub_queries = [user_input]

        return {
            "main_query": user_input,
            "sub_queries": sub_queries
        }

    def retrieve_answers(self, queries_list, top_k=3):
        """é‡å°æ¯å€‹å­å•é¡Œæª¢ç´¢ç›¸é—œæ–‡ä»¶"""
        results = []
        for sub_query in queries_list:
            docs = self.retriever.invoke(sub_query, config=dict(k=top_k))
            retrieved_chunks = [doc.page_content for doc in docs]
            results.append({
                "sub_query": sub_query,
                "chunks": retrieved_chunks
            })
        return results

    def integrate_answers(self, main_query, retrieved_answers):
        """æ•´åˆæª¢ç´¢çµæœä¸¦ç”Ÿæˆæœ€çµ‚å›ç­”"""
        final_prompt = self.PROMPT_TEMPLATE.format(
            main_query=main_query,
            retrieved_answers=retrieved_answers
        )

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.SYSTEM_PROMPT_BRAND},
                {"role": "user", "content": final_prompt},
            ]
        )
        return response.choices[0].message.content

    @modal.wsgi_app()
    def flask_app(self):
        """
        Returns the Flask WSGI application.
        This method is called once per container.
        """
        from flask import Flask, render_template, request, jsonify, Response, stream_with_context
        import json

        web_app = Flask(__name__, static_folder="/app/static", template_folder="/app/templates")
        web_app.config['SECRET_KEY'] = 'mojo-dream-bar-secret-key'

        @web_app.route('/')
        def index():
            """é¦–é """
            return render_template('index.html')

        @web_app.route('/api/chat', methods=['POST'])
        def chat():
            """è™•ç†èŠå¤©è«‹æ±‚"""
            try:
                data = request.get_json()
                user_message = data.get('message', '').strip()

                if not user_message:
                    return jsonify({'error': 'è«‹è¼¸å…¥è¨Šæ¯'}), 400

                # 1. èªæ„æ‹†è§£
                result = self.separate_queries(user_message)
                main_query = result['main_query']
                queries_list = result['sub_queries']

                # 2. å€‹åˆ¥æª¢ç´¢
                retrieved_answers = self.retrieve_answers(queries_list)

                # 3. æ•´åˆå›ç­”
                response = self.integrate_answers(main_query, retrieved_answers)

                return jsonify({
                    'response': response,
                    'status': 'success'
                })

            except Exception as e:
                print(f"éŒ¯èª¤: {str(e)}")
                return jsonify({
                    'error': 'è™•ç†è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦',
                    'status': 'error'
                }), 500

        @web_app.route('/api/chat/stream', methods=['POST'])
        def chat_stream():
            """Streaming chat endpoint with thinking visualization"""
            try:
                data = request.get_json()
                user_message = data.get('message', '').strip()

                if not user_message:
                    return jsonify({'error': 'è«‹è¼¸å…¥è¨Šæ¯'}), 400

                def generate():
                    try:
                        # Stage 1: Query Decomposition
                        result = self.separate_queries(user_message)
                        main_query = result['main_query']
                        queries_list = result['sub_queries']

                        stage1_data = {
                            'type': 'stage1',
                            'data': {
                                'original_query': main_query,
                                'sub_queries': queries_list
                            }
                        }
                        yield f"data: {json.dumps(stage1_data)}\n\n"

                        # Stage 2: Individual Retrieval
                        retrieved_answers = self.retrieve_answers(queries_list)

                        stage2_data = {
                            'type': 'stage2',
                            'data': retrieved_answers
                        }
                        yield f"data: {json.dumps(stage2_data)}\n\n"

                        # Stage 3: Integration Metadata
                        final_prompt = self.PROMPT_TEMPLATE.format(
                            main_query=main_query,
                            retrieved_answers=retrieved_answers
                        )

                        stage3_data = {
                            'type': 'stage3',
                            'data': {
                                'method': 'LLM integration with brand voice',
                                'note': 'Combined all retrieved information',
                                'final_prompt': final_prompt
                            }
                        }
                        yield f"data: {json.dumps(stage3_data)}\n\n"

                        # Final Answer
                        response = self.integrate_answers(main_query, retrieved_answers)

                        final_data = {
                            'type': 'final_answer',
                            'data': response
                        }
                        yield f"data: {json.dumps(final_data)}\n\n"

                        # Stream complete
                        yield f"data: {json.dumps({'type': 'done'})}\n\n"

                    except Exception as e:
                        print(f"Stream error: {str(e)}")
                        error_data = {
                            'type': 'error',
                            'message': 'è™•ç†è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤'
                        }
                        yield f"data: {json.dumps(error_data)}\n\n"

                return Response(
                    stream_with_context(generate()),
                    mimetype='text/event-stream',
                    headers={
                        'Cache-Control': 'no-cache',
                        'X-Accel-Buffering': 'no'
                    }
                )

            except Exception as e:
                print(f"éŒ¯èª¤: {str(e)}")
                return jsonify({
                    'error': 'è™•ç†è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦',
                    'status': 'error'
                }), 500

        return web_app
